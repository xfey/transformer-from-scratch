# Transformer From Scratch

A step-by-step tutorial for implementing Transformer architectures from scratch in PyTorch.

## Overview

This repository provides detailed, well-documented implementations of Transformer architectures. Each implementation includes:

- ğŸ“ Step-by-step explanations
- ğŸ’¡ Detailed comments for every key component
- ğŸ” Mathematical derivations where necessary
- âš¡ Working examples and demonstrations

## Structure

```
transformer_from_scratch/
â”œâ”€â”€ vanilla_transformer/          # ç»å…¸ Transformer ç»“æ„
â”‚   â”œâ”€â”€ attention.py             # æ³¨æ„åŠ›æœºåˆ¶å®ç°
â”‚   â”œâ”€â”€ encoder.py              # ç¼–ç å™¨å®ç°
â”‚   â”œâ”€â”€ decoder.py              # è§£ç å™¨å®ç°
â”‚   â””â”€â”€ transformer.py          # å®Œæ•´æ¨¡å‹
â”‚
â”œâ”€â”€ attention_variants/          # æ³¨æ„åŠ›æœºåˆ¶å˜ä½“
â”‚   â”œâ”€â”€ linear_attention/       # çº¿æ€§æ³¨æ„åŠ›
â”‚   â”œâ”€â”€ sparse_attention/       # ç¨€ç–æ³¨æ„åŠ›
â”‚   â””â”€â”€ efficient_attention/    # é«˜æ•ˆæ³¨æ„åŠ›
â”‚
â”œâ”€â”€ efficient_transformers/      # è½»é‡çº§/é«˜æ•ˆæ¶æ„
â”‚   â”œâ”€â”€ performer/              # Performer
â”‚   â”œâ”€â”€ reformer/              # Reformer
â”‚   â”œâ”€â”€ efficient_vit/         # EfficientViT
â”‚   â””â”€â”€ fast_vit/              # FastViT
â”‚
â”œâ”€â”€ vision_transformers/        # è§†è§‰ Transformer
â”‚   â”œâ”€â”€ vit/                   # Vision Transformer
â”‚   â”œâ”€â”€ swin/                  # Swin Transformer
â”‚   â””â”€â”€ modern_variants/       # æ–°å‹è§†è§‰ Transformer
â”‚
â”œâ”€â”€ specialized/               # ç‰¹å®šä»»åŠ¡æ¶æ„
â”‚   â”œâ”€â”€ detection/            # ç›®æ ‡æ£€æµ‹
â”‚   â””â”€â”€ generation/          # ç”Ÿæˆæ¨¡å‹
â”‚
â””â”€â”€ solutions/                # ç­”æ¡ˆ
```

## Implemented Architectures

| Architecture | Paper | Original Repo | Implementation Path |
|-------------|-------|---------------|-------------------|
| Vanilla Transformer | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor) | `vanilla_transformer/` |
| Linear Attention | [Transformers are RNNs](https://arxiv.org/abs/2006.16236) | [idiap/fast-transformers](https://github.com/idiap/fast-transformers) | `attention_variants/linear_attention/` |
| Performer | [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794) | [google-research/performer](https://github.com/google-research/performer) | `efficient_transformers/performer/` |
| Vision Transformer | [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) | [google-research/vision_transformer](https://github.com/google-research/vision_transformer) | `vision_transformers/vit/` |
| Swin Transformer | [Hierarchical Vision Transformer](https://arxiv.org/abs/2103.14030) | [microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer) | `vision_transformers/swin/` |
| EfficientViT | [EfficientViT](https://arxiv.org/abs/2205.14756) | [microsoft/EfficientViT](https://github.com/microsoft/EfficientViT) | `efficient_transformers/efficient_vit/` |
| DETR | [End-to-End Object Detection](https://arxiv.org/abs/2005.12872) | [facebookresearch/detr](https://github.com/facebookresearch/detr) | `specialized/detection/` |

## Getting Started

1. Clone the repository:
```bash
git clone https://github.com/xfey/transformer-from-scratch.git
cd transformer-from-scratch
```

2. Start with the basics:
   - Read through `vanilla_transformer/attention.py` for attention mechanism
   - Move on to `vanilla_transformer/encoder.py` for full encoder implementation
   - Each file contains detailed explanations and comments

## Learning Path

1. **Classic Transformer**
   - Attention mechanism
   - Encoder & Decoder
   - Full transformer model

2. **Attention Variants**
   - Linear attention
   - Sparse attention
   - Efficient implementations

3. **Modern Architectures**
   - Vision Transformers
   - Efficient models
   - Task-specific variants

## Contributing

Contributions are welcome! Please feel free to:
- Add new implementations
- Improve existing explanations
- Fix bugs or add clarifications
- Suggest new learning paths

## License

This project is licensed under the MIT License - see the LICENSE file for details.
