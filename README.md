# Transformer From Scratch

A step-by-step tutorial for implementing Transformer architectures from scratch in PyTorch.

ğŸš§ This code is under rapid development. æ–¹æ³•åŠæ¨¡å‹æ­£åœ¨å¿«é€Ÿè¡¥å……ä¸­ã€‚ ğŸš§


## å¦‚ä½•å¼€å§‹

1. é˜…è¯»ä»£ç ï¼Œç†è§£æ¨¡å‹ç»“æ„çš„ç†è®ºå®ç°
2. æ ¹æ®ä»£ç ä¸­çš„æç¤ºï¼Œå°è¯•å®ç°æ¨¡å‹ç»“æ„
3. å¯¹æ¯” `solutions` ç›®å½•ä¸‹çš„ä»£ç ï¼Œæ£€æŸ¥ä»£ç æ˜¯å¦æ­£ç¡®

## Getting Started

1. Read the code for each architecture, understand its theoretical basis.
2. Try to implement the code with the hint in the code.
3. Compare your implementation with the provided solution.


## Overview

This repository provides detailed, well-documented implementations of Transformer architectures. Each implementation includes:

- ğŸ“ Step-by-step explanations
- ğŸ’¡ Detailed comments for every key component
- ğŸ” Mathematical derivations where necessary
- âš¡ Working examples and demonstrations

## Structure

```
transformer_from_scratch/
â”œâ”€â”€ vanilla_transformer/          # ç»å…¸ Transformer ç»“æ„
â”‚   â”œâ”€â”€ attention.py             # æ³¨æ„åŠ›æœºåˆ¶å®ç°
â”‚   â”œâ”€â”€ encoder.py              # ç¼–ç å™¨å®ç°
â”‚   â”œâ”€â”€ decoder.py              # è§£ç å™¨å®ç°
â”‚   â””â”€â”€ transformer.py          # å®Œæ•´æ¨¡å‹
â”‚
â”œâ”€â”€ attention_variants/          # æ³¨æ„åŠ›æœºåˆ¶å˜ä½“
â”‚   â””â”€â”€ linear_attention       # çº¿æ€§æ³¨æ„åŠ›
â”‚
â”œâ”€â”€ efficient_transformer/      # è½»é‡çº§/é«˜æ•ˆæ¶æ„
â”‚   â””â”€â”€ performer              # Performer
â”‚
â”œâ”€â”€ vision_transformer/        # è§†è§‰ Transformer
â”‚   â”œâ”€â”€ vit                   # Vision Transformer
â”‚   â””â”€â”€ swin                  # Swin Transformer
â”‚
â””â”€â”€ SOLUTIONS                # ç­”æ¡ˆ
```

## Implemented Architectures

| Architecture | Paper | Original Repo |
|-------------|-------|---------------|
| Vanilla Transformer | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor) |
| Vision Transformer | [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) | [google-research/vision_transformer](https://github.com/google-research/vision_transformer) |
| Swin Transformer | [Hierarchical Vision Transformer](https://arxiv.org/abs/2103.14030) | [microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer) |
| Linear Attention | [Transformers are RNNs](https://arxiv.org/abs/2006.16236) | [idiap/fast-transformers](https://github.com/idiap/fast-transformers) |
| Performer | [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794) | [google-research/performer](https://github.com/google-research/performer) |
| EfficientViT | [EfficientViT](https://arxiv.org/abs/2205.14756) | [microsoft/EfficientViT](https://github.com/microsoft/EfficientViT) |


## Contributing

Contributions are welcome! Please feel free to:
- Add new implementations
- Improve existing explanations
- Suggest new architectures

## License

This project is licensed under the MIT License - see the LICENSE file for details.
